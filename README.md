# url-count
从100GB的文本文件中找到出现次数最多的前100个url，内存限制1GB

# 思路
首先是需要求出每个url在文件中出现的次数，相当于是算一个group by聚合。有两种方式可以做，一种是基于hash的聚合，一种是基于sort的聚合。

基于hash的聚合：
1. 先用一个哈希函数h0将所有的具有相同h0的url进行分组(比如分成100组)，写入磁盘
2. 如果一个组的大小过大(比如大于1GB)，则需要用哈希函数h1对这个组再次进行分组
3. 分组完成后，相同的url必然在同一个组中，则针对每个组，利用内存中的hashmap计算聚合函数

基于sort的聚合：
1. 按顺序分块读入url，在内存中对每一块进行排序，然后分块写入磁盘
2. 利用堆将这些内部有序的块合并成总体有序的序列，相同的url会排在一起，迭代计算聚合函数

这两种方式都是总共需要读两次磁盘，写一次磁盘

# 实现

由于感觉sort-aggregate的内存使用相对于hash-aggregate来说比较可控，所以选择了实现基于sort的聚合。

